{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lorenzo-Siviero-5753818/LSiviero-DataScience-GenAI-Submissions/blob/main/Assignment_5/6_02_DNN_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1xqQczl0FG-qtNA2_WQYuWePW9oU8irqJ)"
      ],
      "metadata": {
        "id": "E0T9_-jFXxxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.02 Dense Neural Network (with PyTorch)\n",
        "This will expand on our logistic regression example and take us through building our first neural network. If you haven't already, be sure to check (and if neccessary) switch to GPU processing by clicking Runtime > Change runtime type and selecting GPU. We can test this has worked with the following code:"
      ],
      "metadata": {
        "id": "dcEWDwlu94Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "print(\"Num GPUs Available: \", torch.cuda.device_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8cIpNbCvuQA",
        "outputId": "a021c74e-8fac-42b2-bd5c-15268358a3e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully your code shows you have 1 GPU available! Next let's get some data. We'll start with another in-built dataset:"
      ],
      "metadata": {
        "id": "8d6FF1wK-ph8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload an in-built Python (OK semi-in-built) dataset\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# import the data\n",
        "data = load_diabetes()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MziWWXu-0ur",
        "outputId": "57dc2b46-27be-4bc5-a92b-c198f2e2daba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
              "          0.01990749, -0.01764613],\n",
              "        [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
              "         -0.06833155, -0.09220405],\n",
              "        [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
              "          0.00286131, -0.02593034],\n",
              "        ...,\n",
              "        [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
              "         -0.04688253,  0.01549073],\n",
              "        [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
              "          0.04452873, -0.02593034],\n",
              "        [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
              "         -0.00422151,  0.00306441]]),\n",
              " 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
              "         69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
              "         68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
              "         87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
              "        259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
              "        128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
              "        150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
              "        200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
              "         42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
              "         83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
              "        104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
              "        173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
              "        107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
              "         60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
              "        197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
              "         59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
              "        237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
              "        143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
              "        142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
              "         77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
              "         78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
              "        154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
              "         71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
              "        150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
              "        145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
              "         94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
              "         60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
              "         31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
              "        114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
              "        191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
              "        244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
              "        263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
              "         77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
              "         58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
              "        140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
              "        219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
              "         43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
              "        140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
              "         84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
              "         94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
              "        220.,  57.]),\n",
              " 'frame': None,\n",
              " 'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 442\\n\\n:Number of Attributes: First 10 columns are numeric predictive values\\n\\n:Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n:Attribute Information:\\n    - age     age in years\\n    - sex\\n    - bmi     body mass index\\n    - bp      average blood pressure\\n    - s1      tc, total serum cholesterol\\n    - s2      ldl, low-density lipoproteins\\n    - s3      hdl, high-density lipoproteins\\n    - s4      tch, total cholesterol / HDL\\n    - s5      ltg, possibly log of serum triglycerides level\\n    - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\\n',\n",
              " 'feature_names': ['age',\n",
              "  'sex',\n",
              "  'bmi',\n",
              "  'bp',\n",
              "  's1',\n",
              "  's2',\n",
              "  's3',\n",
              "  's4',\n",
              "  's5',\n",
              "  's6'],\n",
              " 'data_filename': 'diabetes_data_raw.csv.gz',\n",
              " 'target_filename': 'diabetes_target.csv.gz',\n",
              " 'data_module': 'sklearn.datasets.data'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are working on a regression problem, with \"structured\" data which has already been cleaned and normalised. We can skip the usual cleaning/engineering steps. However, we do need to get the data into PyTorch:"
      ],
      "metadata": {
        "id": "cZKrbx70_cIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X = torch.tensor(data.data, dtype=torch.float32)\n",
        "y = torch.tensor(data.target, dtype=torch.float32).reshape(-1, 1) # Reshape y to be a column vector"
      ],
      "metadata": {
        "id": "f9PHiljr73fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our data is stored in tensors we can do train/test splitting as before (in fact we can use sklearn as before):"
      ],
      "metadata": {
        "id": "hu8VH2_SAOoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYJN01DV8Fac",
        "outputId": "dc26d4bd-fb1f-4c60-c570-644634bba6f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([353, 10]) torch.Size([353, 1])\n",
            "torch.Size([89, 10]) torch.Size([89, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can set up our batches for training. As we have a nice round 400 let's go with batches of 50 (8 batches in total). We'll also seperate the features and labels:"
      ],
      "metadata": {
        "id": "LKmbZoCrJijU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create TensorDatasets and DataLoaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)"
      ],
      "metadata": {
        "id": "de0uOko08d-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now its time to build our model. We'll keep it simple ... a model with an input layer of 10 features and then 2x _Dense_ (fully connected) layers each with 5 neurons and ReLU activation. Our output layer will be size=1 given this is a regression problem and we want a single value output per prediction.\n",
        "\n",
        "This will be easier to understand if you have read through the logistic regression tutorial."
      ],
      "metadata": {
        "id": "yCCG8kKHCVnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the model\n",
        "class DiabetesModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiabetesModel, self).__init__()\n",
        "        # we'll set up the layers as a sequence using nn.Sequential\n",
        "        self.layers = nn.Sequential(\n",
        "\n",
        "            # first layer will be a linear layer that has 5x neurons\n",
        "            # (5x sets of linear regression)\n",
        "            # the layer takes the 10 features as input (i.e. 10, 5)\n",
        "            nn.Linear(10, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU activation\n",
        "\n",
        "            # second linear layer again has 5 neurons\n",
        "            # this time taking the input as the output of the last layer\n",
        "            # (which had 5x neurons)\n",
        "            nn.Linear(5, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU again\n",
        "\n",
        "            # last linear layer takes the output from the previous 5 neurons\n",
        "            # this time its a single output with no activation\n",
        "            # i.e. this is the predicitons (regression)\n",
        "            nn.Linear(5, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x) # pass the data through the layers"
      ],
      "metadata": {
        "id": "844H60hcCV3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before we need to create a model object, specify the loss (criterion) and an optimiser (which we cover next week):"
      ],
      "metadata": {
        "id": "cv4-loCz91aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = DiabetesModel()\n",
        "criterion = nn.MSELoss() # MSE loss function\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "EPx_Wy6g9uA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train the model. Again, the logistic regression tutorial (6.01) may help you undertstand this:"
      ],
      "metadata": {
        "id": "HOKfjkfW-Ish"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop (example - you'll likely want to add more epochs)\n",
        "epochs = 100 # 100 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # use the train_loader to pass the inputs (x) and targets (y)\n",
        "  for inputs, targets in train_loader:\n",
        "    # pass to the GPU (hopefully)\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # pass model to GPU as well\n",
        "    model.to(device)\n",
        "\n",
        "    model.train() # put the model object in train mode\n",
        "    optimiser.zero_grad() # reset the gradiants\n",
        "    outputs = model(inputs) # create outputs\n",
        "    loss = criterion(outputs, targets) # compare with Y to get loss\n",
        "    loss.backward() # backpropogate the loss (next week)\n",
        "    optimiser.step() # # update the parameters based on this round of training\n",
        "\n",
        "  # every 10 steps we will print out the current loss\n",
        "    if (epoch+1) % 10 == 0: # modular arithmetic\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtMUgfwT-HGt",
        "outputId": "37d80a16-4b1a-4fe2-c64a-4f038ac67ef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 35494.1602\n",
            "Epoch [10/100], Loss: 30528.4902\n",
            "Epoch [10/100], Loss: 32816.6875\n",
            "Epoch [10/100], Loss: 28316.4688\n",
            "Epoch [10/100], Loss: 33928.7578\n",
            "Epoch [10/100], Loss: 22381.1426\n",
            "Epoch [10/100], Loss: 22674.5371\n",
            "Epoch [10/100], Loss: 28139.6934\n",
            "Epoch [20/100], Loss: 24963.6797\n",
            "Epoch [20/100], Loss: 24961.707\n",
            "Epoch [20/100], Loss: 32257.2852\n",
            "Epoch [20/100], Loss: 35983.5586\n",
            "Epoch [20/100], Loss: 32344.2227\n",
            "Epoch [20/100], Loss: 29714.6934\n",
            "Epoch [20/100], Loss: 25721.7188\n",
            "Epoch [20/100], Loss: 12764.1973\n",
            "Epoch [30/100], Loss: 39235.1133\n",
            "Epoch [30/100], Loss: 28568.8398\n",
            "Epoch [30/100], Loss: 34221.4883\n",
            "Epoch [30/100], Loss: 32108.6895\n",
            "Epoch [30/100], Loss: 24917.9844\n",
            "Epoch [30/100], Loss: 25557.209\n",
            "Epoch [30/100], Loss: 18591.5996\n",
            "Epoch [30/100], Loss: 28623.7773\n",
            "Epoch [40/100], Loss: 26196.1562\n",
            "Epoch [40/100], Loss: 25400.6152\n",
            "Epoch [40/100], Loss: 31947.25\n",
            "Epoch [40/100], Loss: 27101.8418\n",
            "Epoch [40/100], Loss: 28143.4609\n",
            "Epoch [40/100], Loss: 31934.5059\n",
            "Epoch [40/100], Loss: 28412.0391\n",
            "Epoch [40/100], Loss: 49857.8516\n",
            "Epoch [50/100], Loss: 25657.0566\n",
            "Epoch [50/100], Loss: 27104.0117\n",
            "Epoch [50/100], Loss: 28740.9121\n",
            "Epoch [50/100], Loss: 29938.7344\n",
            "Epoch [50/100], Loss: 29541.9043\n",
            "Epoch [50/100], Loss: 23955.6738\n",
            "Epoch [50/100], Loss: 31367.7676\n",
            "Epoch [50/100], Loss: 30608.7324\n",
            "Epoch [60/100], Loss: 22798.7402\n",
            "Epoch [60/100], Loss: 28524.8496\n",
            "Epoch [60/100], Loss: 27820.9043\n",
            "Epoch [60/100], Loss: 24150.0703\n",
            "Epoch [60/100], Loss: 30266.3398\n",
            "Epoch [60/100], Loss: 27209.0898\n",
            "Epoch [60/100], Loss: 30313.1211\n",
            "Epoch [60/100], Loss: 25040.1211\n",
            "Epoch [70/100], Loss: 24213.1016\n",
            "Epoch [70/100], Loss: 27541.6484\n",
            "Epoch [70/100], Loss: 26437.207\n",
            "Epoch [70/100], Loss: 20962.0566\n",
            "Epoch [70/100], Loss: 28473.0078\n",
            "Epoch [70/100], Loss: 29140.9688\n",
            "Epoch [70/100], Loss: 26849.7793\n",
            "Epoch [70/100], Loss: 32079.1562\n",
            "Epoch [80/100], Loss: 26994.8828\n",
            "Epoch [80/100], Loss: 25197.334\n",
            "Epoch [80/100], Loss: 23056.2188\n",
            "Epoch [80/100], Loss: 30617.9688\n",
            "Epoch [80/100], Loss: 19684.5664\n",
            "Epoch [80/100], Loss: 20726.4512\n",
            "Epoch [80/100], Loss: 29127.4238\n",
            "Epoch [80/100], Loss: 23758.3262\n",
            "Epoch [90/100], Loss: 26575.252\n",
            "Epoch [90/100], Loss: 17695.0762\n",
            "Epoch [90/100], Loss: 23576.832\n",
            "Epoch [90/100], Loss: 22490.0898\n",
            "Epoch [90/100], Loss: 24183.2891\n",
            "Epoch [90/100], Loss: 24281.9141\n",
            "Epoch [90/100], Loss: 24573.3438\n",
            "Epoch [90/100], Loss: 48672.8828\n",
            "Epoch [100/100], Loss: 17893.6914\n",
            "Epoch [100/100], Loss: 21888.875\n",
            "Epoch [100/100], Loss: 19425.6641\n",
            "Epoch [100/100], Loss: 28905.5586\n",
            "Epoch [100/100], Loss: 24394.7148\n",
            "Epoch [100/100], Loss: 19161.9004\n",
            "Epoch [100/100], Loss: 19263.9023\n",
            "Epoch [100/100], Loss: 31666.4102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see loss is significantly lower at the end than it was at the start. However, it is also bouncing around a little still which suggests the model needs more training (100 epochs is not a lot in deep learning terms). However, let's evaluate as before:"
      ],
      "metadata": {
        "id": "E72ZTKSqAODE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (example)\n",
        "model.eval() # testing mode\n",
        "mse_values = [] # collect the MSE scores\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs) # predict the test data\n",
        "\n",
        "        # Calculate Mean Squared Error\n",
        "        mse = criterion(outputs, targets) # calcualte mse for the batch\n",
        "        mse_values.append(mse.item()) # add to the list of MSE values\n",
        "\n",
        "# Calculate and print the average MSE\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbuAH6p8A-Vh",
        "outputId": "0fba5271-aa14-43a1-e5f8-319a5f078092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 18670.296875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE looks expected given training (no obvious sign of overfitting). However, we probably can get better results with tuning and more epochs.\n",
        "\n",
        "Let's run the loop again a little differently to collect the predicted values (y_hat) and actuals (y) and add them to a dataset for comparions:"
      ],
      "metadata": {
        "id": "HQ26bA08Up12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({'Predicted': np.array(predictions).flatten(), 'Actual': np.array(actuals).flatten()})\n",
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "8AYsDDSLUp_u",
        "outputId": "9db13a1d-cfac-4b4d-ae84-380a3ef00b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Predicted  Actual\n",
              "0   29.994797   219.0\n",
              "1   28.378031    70.0\n",
              "2   29.565075   202.0\n",
              "3   36.472412   230.0\n",
              "4   28.333435   111.0\n",
              "..        ...     ...\n",
              "84  26.088142   153.0\n",
              "85  24.193642    98.0\n",
              "86  22.075033    37.0\n",
              "87  22.611799    63.0\n",
              "88  26.582020   184.0\n",
              "\n",
              "[89 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-90e65730-ae00-472b-b287-8c452db4b889\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>29.994797</td>\n",
              "      <td>219.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>28.378031</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29.565075</td>\n",
              "      <td>202.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>36.472412</td>\n",
              "      <td>230.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28.333435</td>\n",
              "      <td>111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>26.088142</td>\n",
              "      <td>153.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>24.193642</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>22.075033</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>22.611799</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>26.582020</td>\n",
              "      <td>184.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90e65730-ae00-472b-b287-8c452db4b889')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-90e65730-ae00-472b-b287-8c452db4b889 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-90e65730-ae00-472b-b287-8c452db4b889');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7ca21758-ea4a-4e3a-8050-bbedefd1bc64\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7ca21758-ea4a-4e3a-8050-bbedefd1bc64')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7ca21758-ea4a-4e3a-8050-bbedefd1bc64 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_935da9b1-f87a-4cbb-ad11-617735e1ca6e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_935da9b1-f87a-4cbb-ad11-617735e1ca6e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 89,\n  \"fields\": [\n    {\n      \"column\": \"Predicted\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 89,\n        \"samples\": [\n          29.550235748291016,\n          25.84369468688965,\n          29.453479766845703\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          111.0,\n          61.0,\n          252.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Side-by-side, they don't look great. Can you improve them?\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## EXERCISE #1\n",
        "Try increasing the number of epochs to 1,000 (when the model is fairly well trained then the results printed for each 10x epochs will be fairly stable and not change much). Does this give better results?\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## EXERCISE #2 (optional)\n",
        "Try experimenting with the architecture (number of neurons and/or number of layers). Can we reach an optimal architecture?"
      ],
      "metadata": {
        "id": "LDcM98lHbgP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop (example - you'll likely want to add more epochs)\n",
        "epochs = 1000 # 100 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # use the train_loader to pass the inputs (x) and targets (y)\n",
        "  for inputs, targets in train_loader:\n",
        "    # pass to the GPU (hopefully)\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # pass model to GPU as well\n",
        "    model.to(device)\n",
        "\n",
        "    model.train() # put the model object in train mode\n",
        "    optimiser.zero_grad() # reset the gradiants\n",
        "    outputs = model(inputs) # create outputs\n",
        "    loss = criterion(outputs, targets) # compare with Y to get loss\n",
        "    loss.backward() # backpropogate the loss (next week)\n",
        "    optimiser.step() # # update the parameters based on this round of training\n",
        "\n",
        "  # every 10 steps we will print out the current loss\n",
        "    if (epoch+1) % 10 == 0: # modular arithmetic\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVbrgfIHPZ1p",
        "outputId": "0cbec0a4-a1c5-4c4d-e51f-2814d96fdd30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 22680.9551\n",
            "Epoch [10/1000], Loss: 18903.5684\n",
            "Epoch [10/1000], Loss: 18440.5527\n",
            "Epoch [10/1000], Loss: 22136.9199\n",
            "Epoch [10/1000], Loss: 16158.9717\n",
            "Epoch [10/1000], Loss: 18319.0234\n",
            "Epoch [10/1000], Loss: 17869.3672\n",
            "Epoch [10/1000], Loss: 48381.9492\n",
            "Epoch [20/1000], Loss: 17065.2754\n",
            "Epoch [20/1000], Loss: 16954.2852\n",
            "Epoch [20/1000], Loss: 16939.1719\n",
            "Epoch [20/1000], Loss: 14889.5469\n",
            "Epoch [20/1000], Loss: 23030.5273\n",
            "Epoch [20/1000], Loss: 11794.2451\n",
            "Epoch [20/1000], Loss: 19404.1758\n",
            "Epoch [20/1000], Loss: 16742.125\n",
            "Epoch [30/1000], Loss: 12993.5811\n",
            "Epoch [30/1000], Loss: 15099.6934\n",
            "Epoch [30/1000], Loss: 15483.7725\n",
            "Epoch [30/1000], Loss: 10877.4326\n",
            "Epoch [30/1000], Loss: 16818.002\n",
            "Epoch [30/1000], Loss: 17316.6055\n",
            "Epoch [30/1000], Loss: 15627.6348\n",
            "Epoch [30/1000], Loss: 14918.5801\n",
            "Epoch [40/1000], Loss: 12624.1797\n",
            "Epoch [40/1000], Loss: 11023.5908\n",
            "Epoch [40/1000], Loss: 13281.0049\n",
            "Epoch [40/1000], Loss: 11322.5146\n",
            "Epoch [40/1000], Loss: 12587.9424\n",
            "Epoch [40/1000], Loss: 14635.1074\n",
            "Epoch [40/1000], Loss: 12678.1289\n",
            "Epoch [40/1000], Loss: 18890.3926\n",
            "Epoch [50/1000], Loss: 12719.4668\n",
            "Epoch [50/1000], Loss: 9966.8232\n",
            "Epoch [50/1000], Loss: 10663.5801\n",
            "Epoch [50/1000], Loss: 15112.1611\n",
            "Epoch [50/1000], Loss: 10565.9688\n",
            "Epoch [50/1000], Loss: 7742.4585\n",
            "Epoch [50/1000], Loss: 7881.3423\n",
            "Epoch [50/1000], Loss: 10248.707\n",
            "Epoch [60/1000], Loss: 9677.7412\n",
            "Epoch [60/1000], Loss: 8466.29\n",
            "Epoch [60/1000], Loss: 8809.8184\n",
            "Epoch [60/1000], Loss: 7719.3799\n",
            "Epoch [60/1000], Loss: 11011.541\n",
            "Epoch [60/1000], Loss: 9532.3203\n",
            "Epoch [60/1000], Loss: 7400.6968\n",
            "Epoch [60/1000], Loss: 6872.9111\n",
            "Epoch [70/1000], Loss: 7783.6499\n",
            "Epoch [70/1000], Loss: 7607.2222\n",
            "Epoch [70/1000], Loss: 8012.3447\n",
            "Epoch [70/1000], Loss: 6928.8428\n",
            "Epoch [70/1000], Loss: 6617.3247\n",
            "Epoch [70/1000], Loss: 9368.3721\n",
            "Epoch [70/1000], Loss: 6589.5088\n",
            "Epoch [70/1000], Loss: 1755.915\n",
            "Epoch [80/1000], Loss: 8423.0234\n",
            "Epoch [80/1000], Loss: 5837.186\n",
            "Epoch [80/1000], Loss: 4494.1523\n",
            "Epoch [80/1000], Loss: 7009.9473\n",
            "Epoch [80/1000], Loss: 7868.9873\n",
            "Epoch [80/1000], Loss: 6742.1831\n",
            "Epoch [80/1000], Loss: 4838.9326\n",
            "Epoch [80/1000], Loss: 389.8822\n",
            "Epoch [90/1000], Loss: 4601.6372\n",
            "Epoch [90/1000], Loss: 6551.1235\n",
            "Epoch [90/1000], Loss: 5758.0449\n",
            "Epoch [90/1000], Loss: 6176.6313\n",
            "Epoch [90/1000], Loss: 3845.8005\n",
            "Epoch [90/1000], Loss: 7045.1138\n",
            "Epoch [90/1000], Loss: 5356.2666\n",
            "Epoch [90/1000], Loss: 4814.9219\n",
            "Epoch [100/1000], Loss: 6284.2925\n",
            "Epoch [100/1000], Loss: 6275.4043\n",
            "Epoch [100/1000], Loss: 5110.251\n",
            "Epoch [100/1000], Loss: 4409.6406\n",
            "Epoch [100/1000], Loss: 3473.0837\n",
            "Epoch [100/1000], Loss: 6032.3579\n",
            "Epoch [100/1000], Loss: 3852.9023\n",
            "Epoch [100/1000], Loss: 363.4824\n",
            "Epoch [110/1000], Loss: 5378.4941\n",
            "Epoch [110/1000], Loss: 3621.103\n",
            "Epoch [110/1000], Loss: 5545.48\n",
            "Epoch [110/1000], Loss: 4198.3325\n",
            "Epoch [110/1000], Loss: 4230.7559\n",
            "Epoch [110/1000], Loss: 5142.8691\n",
            "Epoch [110/1000], Loss: 4364.2026\n",
            "Epoch [110/1000], Loss: 5903.2539\n",
            "Epoch [120/1000], Loss: 4157.062\n",
            "Epoch [120/1000], Loss: 6017.5063\n",
            "Epoch [120/1000], Loss: 3912.7288\n",
            "Epoch [120/1000], Loss: 4058.7065\n",
            "Epoch [120/1000], Loss: 4035.8979\n",
            "Epoch [120/1000], Loss: 5072.71\n",
            "Epoch [120/1000], Loss: 3899.1074\n",
            "Epoch [120/1000], Loss: 4068.9763\n",
            "Epoch [130/1000], Loss: 3861.3923\n",
            "Epoch [130/1000], Loss: 3624.0818\n",
            "Epoch [130/1000], Loss: 4090.2351\n",
            "Epoch [130/1000], Loss: 4693.9868\n",
            "Epoch [130/1000], Loss: 4761.0938\n",
            "Epoch [130/1000], Loss: 4152.7705\n",
            "Epoch [130/1000], Loss: 5119.6738\n",
            "Epoch [130/1000], Loss: 625.387\n",
            "Epoch [140/1000], Loss: 4305.7139\n",
            "Epoch [140/1000], Loss: 3534.79\n",
            "Epoch [140/1000], Loss: 4830.6772\n",
            "Epoch [140/1000], Loss: 4657.3672\n",
            "Epoch [140/1000], Loss: 3530.3433\n",
            "Epoch [140/1000], Loss: 4398.9756\n",
            "Epoch [140/1000], Loss: 3946.8787\n",
            "Epoch [140/1000], Loss: 8651.7246\n",
            "Epoch [150/1000], Loss: 4971.7871\n",
            "Epoch [150/1000], Loss: 3776.7581\n",
            "Epoch [150/1000], Loss: 3284.7371\n",
            "Epoch [150/1000], Loss: 3956.9336\n",
            "Epoch [150/1000], Loss: 4002.5569\n",
            "Epoch [150/1000], Loss: 4426.8965\n",
            "Epoch [150/1000], Loss: 4661.7197\n",
            "Epoch [150/1000], Loss: 2817.6787\n",
            "Epoch [160/1000], Loss: 3827.9717\n",
            "Epoch [160/1000], Loss: 3356.583\n",
            "Epoch [160/1000], Loss: 3990.7478\n",
            "Epoch [160/1000], Loss: 3634.4624\n",
            "Epoch [160/1000], Loss: 4990.0107\n",
            "Epoch [160/1000], Loss: 4852.2183\n",
            "Epoch [160/1000], Loss: 4042.4849\n",
            "Epoch [160/1000], Loss: 3051.5417\n",
            "Epoch [170/1000], Loss: 4547.2866\n",
            "Epoch [170/1000], Loss: 4085.8899\n",
            "Epoch [170/1000], Loss: 3515.2749\n",
            "Epoch [170/1000], Loss: 3723.0593\n",
            "Epoch [170/1000], Loss: 3690.6245\n",
            "Epoch [170/1000], Loss: 4318.4619\n",
            "Epoch [170/1000], Loss: 4418.6504\n",
            "Epoch [170/1000], Loss: 4517.0029\n",
            "Epoch [180/1000], Loss: 3842.2874\n",
            "Epoch [180/1000], Loss: 4415.6387\n",
            "Epoch [180/1000], Loss: 3858.1956\n",
            "Epoch [180/1000], Loss: 3000.9995\n",
            "Epoch [180/1000], Loss: 4599.731\n",
            "Epoch [180/1000], Loss: 3820.4377\n",
            "Epoch [180/1000], Loss: 4365.9941\n",
            "Epoch [180/1000], Loss: 6397.1943\n",
            "Epoch [190/1000], Loss: 3137.8765\n",
            "Epoch [190/1000], Loss: 4849.3516\n",
            "Epoch [190/1000], Loss: 3522.5237\n",
            "Epoch [190/1000], Loss: 3966.2729\n",
            "Epoch [190/1000], Loss: 3677.49\n",
            "Epoch [190/1000], Loss: 4793.0635\n",
            "Epoch [190/1000], Loss: 3754.4478\n",
            "Epoch [190/1000], Loss: 5542.2417\n",
            "Epoch [200/1000], Loss: 3397.0811\n",
            "Epoch [200/1000], Loss: 4166.1191\n",
            "Epoch [200/1000], Loss: 4178.5732\n",
            "Epoch [200/1000], Loss: 4465.5654\n",
            "Epoch [200/1000], Loss: 3889.4944\n",
            "Epoch [200/1000], Loss: 3918.0212\n",
            "Epoch [200/1000], Loss: 3548.3943\n",
            "Epoch [200/1000], Loss: 3953.9639\n",
            "Epoch [210/1000], Loss: 4558.8481\n",
            "Epoch [210/1000], Loss: 4190.8882\n",
            "Epoch [210/1000], Loss: 3118.2837\n",
            "Epoch [210/1000], Loss: 3587.2068\n",
            "Epoch [210/1000], Loss: 4437.3047\n",
            "Epoch [210/1000], Loss: 2814.25\n",
            "Epoch [210/1000], Loss: 4765.7837\n",
            "Epoch [210/1000], Loss: 1608.7996\n",
            "Epoch [220/1000], Loss: 3689.1475\n",
            "Epoch [220/1000], Loss: 3394.6345\n",
            "Epoch [220/1000], Loss: 3678.45\n",
            "Epoch [220/1000], Loss: 3744.582\n",
            "Epoch [220/1000], Loss: 4712.6567\n",
            "Epoch [220/1000], Loss: 4013.2617\n",
            "Epoch [220/1000], Loss: 3871.5542\n",
            "Epoch [220/1000], Loss: 4069.6909\n",
            "Epoch [230/1000], Loss: 3903.5945\n",
            "Epoch [230/1000], Loss: 4117.98\n",
            "Epoch [230/1000], Loss: 4507.3491\n",
            "Epoch [230/1000], Loss: 2463.7278\n",
            "Epoch [230/1000], Loss: 3638.9849\n",
            "Epoch [230/1000], Loss: 3832.9949\n",
            "Epoch [230/1000], Loss: 4501.6494\n",
            "Epoch [230/1000], Loss: 2619.063\n",
            "Epoch [240/1000], Loss: 3998.9065\n",
            "Epoch [240/1000], Loss: 3901.7551\n",
            "Epoch [240/1000], Loss: 3745.0305\n",
            "Epoch [240/1000], Loss: 4177.9448\n",
            "Epoch [240/1000], Loss: 4806.4648\n",
            "Epoch [240/1000], Loss: 2720.4812\n",
            "Epoch [240/1000], Loss: 3381.5361\n",
            "Epoch [240/1000], Loss: 2807.5918\n",
            "Epoch [250/1000], Loss: 3644.6111\n",
            "Epoch [250/1000], Loss: 4249.8701\n",
            "Epoch [250/1000], Loss: 3421.958\n",
            "Epoch [250/1000], Loss: 3204.4309\n",
            "Epoch [250/1000], Loss: 3898.1479\n",
            "Epoch [250/1000], Loss: 4656.9673\n",
            "Epoch [250/1000], Loss: 3455.7637\n",
            "Epoch [250/1000], Loss: 2536.2825\n",
            "Epoch [260/1000], Loss: 3364.2058\n",
            "Epoch [260/1000], Loss: 3666.5698\n",
            "Epoch [260/1000], Loss: 3323.8125\n",
            "Epoch [260/1000], Loss: 3322.7432\n",
            "Epoch [260/1000], Loss: 4215.3486\n",
            "Epoch [260/1000], Loss: 3356.3718\n",
            "Epoch [260/1000], Loss: 5004.9297\n",
            "Epoch [260/1000], Loss: 3632.2437\n",
            "Epoch [270/1000], Loss: 3157.6343\n",
            "Epoch [270/1000], Loss: 3513.5327\n",
            "Epoch [270/1000], Loss: 4182.4331\n",
            "Epoch [270/1000], Loss: 3192.8909\n",
            "Epoch [270/1000], Loss: 4071.1162\n",
            "Epoch [270/1000], Loss: 3899.5149\n",
            "Epoch [270/1000], Loss: 3646.0874\n",
            "Epoch [270/1000], Loss: 10084.7988\n",
            "Epoch [280/1000], Loss: 4029.8706\n",
            "Epoch [280/1000], Loss: 3126.5383\n",
            "Epoch [280/1000], Loss: 3320.8086\n",
            "Epoch [280/1000], Loss: 4209.5024\n",
            "Epoch [280/1000], Loss: 3222.4143\n",
            "Epoch [280/1000], Loss: 4864.5391\n",
            "Epoch [280/1000], Loss: 3034.9324\n",
            "Epoch [280/1000], Loss: 4532.1494\n",
            "Epoch [290/1000], Loss: 4468.5747\n",
            "Epoch [290/1000], Loss: 2780.3477\n",
            "Epoch [290/1000], Loss: 3508.7725\n",
            "Epoch [290/1000], Loss: 3651.1011\n",
            "Epoch [290/1000], Loss: 4468.8789\n",
            "Epoch [290/1000], Loss: 3541.3821\n",
            "Epoch [290/1000], Loss: 3301.2344\n",
            "Epoch [290/1000], Loss: 2975.7026\n",
            "Epoch [300/1000], Loss: 4890.7153\n",
            "Epoch [300/1000], Loss: 3477.113\n",
            "Epoch [300/1000], Loss: 3225.0325\n",
            "Epoch [300/1000], Loss: 3075.7322\n",
            "Epoch [300/1000], Loss: 3379.1255\n",
            "Epoch [300/1000], Loss: 3160.0093\n",
            "Epoch [300/1000], Loss: 4313.3906\n",
            "Epoch [300/1000], Loss: 3331.6318\n",
            "Epoch [310/1000], Loss: 2919.9719\n",
            "Epoch [310/1000], Loss: 4112.209\n",
            "Epoch [310/1000], Loss: 3364.5742\n",
            "Epoch [310/1000], Loss: 3202.1301\n",
            "Epoch [310/1000], Loss: 3853.4617\n",
            "Epoch [310/1000], Loss: 3773.7981\n",
            "Epoch [310/1000], Loss: 4283.6094\n",
            "Epoch [310/1000], Loss: 918.8087\n",
            "Epoch [320/1000], Loss: 3593.1018\n",
            "Epoch [320/1000], Loss: 3018.9556\n",
            "Epoch [320/1000], Loss: 3203.5337\n",
            "Epoch [320/1000], Loss: 4643.166\n",
            "Epoch [320/1000], Loss: 4308.9385\n",
            "Epoch [320/1000], Loss: 2811.5225\n",
            "Epoch [320/1000], Loss: 3631.5908\n",
            "Epoch [320/1000], Loss: 2796.7129\n",
            "Epoch [330/1000], Loss: 3057.7639\n",
            "Epoch [330/1000], Loss: 4143.1943\n",
            "Epoch [330/1000], Loss: 3648.9199\n",
            "Epoch [330/1000], Loss: 2527.9988\n",
            "Epoch [330/1000], Loss: 3625.3013\n",
            "Epoch [330/1000], Loss: 3651.3125\n",
            "Epoch [330/1000], Loss: 4108.7998\n",
            "Epoch [330/1000], Loss: 7673.8374\n",
            "Epoch [340/1000], Loss: 3057.553\n",
            "Epoch [340/1000], Loss: 3229.7468\n",
            "Epoch [340/1000], Loss: 3939.635\n",
            "Epoch [340/1000], Loss: 4148.769\n",
            "Epoch [340/1000], Loss: 4693.4854\n",
            "Epoch [340/1000], Loss: 2529.1765\n",
            "Epoch [340/1000], Loss: 3317.7837\n",
            "Epoch [340/1000], Loss: 2525.2529\n",
            "Epoch [350/1000], Loss: 3933.4448\n",
            "Epoch [350/1000], Loss: 3078.6228\n",
            "Epoch [350/1000], Loss: 3567.1311\n",
            "Epoch [350/1000], Loss: 3262.6699\n",
            "Epoch [350/1000], Loss: 3236.8582\n",
            "Epoch [350/1000], Loss: 3694.5049\n",
            "Epoch [350/1000], Loss: 3998.5325\n",
            "Epoch [350/1000], Loss: 2301.6443\n",
            "Epoch [360/1000], Loss: 3698.0784\n",
            "Epoch [360/1000], Loss: 4613.4277\n",
            "Epoch [360/1000], Loss: 2257.5664\n",
            "Epoch [360/1000], Loss: 3624.6841\n",
            "Epoch [360/1000], Loss: 3229.1553\n",
            "Epoch [360/1000], Loss: 3251.28\n",
            "Epoch [360/1000], Loss: 4021.0974\n",
            "Epoch [360/1000], Loss: 1053.1177\n",
            "Epoch [370/1000], Loss: 3828.9966\n",
            "Epoch [370/1000], Loss: 2821.1667\n",
            "Epoch [370/1000], Loss: 2991.0808\n",
            "Epoch [370/1000], Loss: 3852.7466\n",
            "Epoch [370/1000], Loss: 3463.1587\n",
            "Epoch [370/1000], Loss: 4469.1416\n",
            "Epoch [370/1000], Loss: 2881.5317\n",
            "Epoch [370/1000], Loss: 5049.3555\n",
            "Epoch [380/1000], Loss: 3731.0193\n",
            "Epoch [380/1000], Loss: 4121.562\n",
            "Epoch [380/1000], Loss: 3231.0894\n",
            "Epoch [380/1000], Loss: 3406.4717\n",
            "Epoch [380/1000], Loss: 3241.2278\n",
            "Epoch [380/1000], Loss: 3463.7883\n",
            "Epoch [380/1000], Loss: 3105.6167\n",
            "Epoch [380/1000], Loss: 2799.2261\n",
            "Epoch [390/1000], Loss: 3422.8787\n",
            "Epoch [390/1000], Loss: 3451.5703\n",
            "Epoch [390/1000], Loss: 3509.314\n",
            "Epoch [390/1000], Loss: 3679.9287\n",
            "Epoch [390/1000], Loss: 3477.9937\n",
            "Epoch [390/1000], Loss: 3520.3438\n",
            "Epoch [390/1000], Loss: 2957.0659\n",
            "Epoch [390/1000], Loss: 5217.5874\n",
            "Epoch [400/1000], Loss: 2932.8508\n",
            "Epoch [400/1000], Loss: 3380.8193\n",
            "Epoch [400/1000], Loss: 2584.3252\n",
            "Epoch [400/1000], Loss: 4946.6157\n",
            "Epoch [400/1000], Loss: 3945.1624\n",
            "Epoch [400/1000], Loss: 3675.9363\n",
            "Epoch [400/1000], Loss: 2671.2422\n",
            "Epoch [400/1000], Loss: 1002.6809\n",
            "Epoch [410/1000], Loss: 2661.5059\n",
            "Epoch [410/1000], Loss: 3144.188\n",
            "Epoch [410/1000], Loss: 4386.0635\n",
            "Epoch [410/1000], Loss: 4026.8315\n",
            "Epoch [410/1000], Loss: 2773.1287\n",
            "Epoch [410/1000], Loss: 3986.7593\n",
            "Epoch [410/1000], Loss: 2897.5205\n",
            "Epoch [410/1000], Loss: 3410.7036\n",
            "Epoch [420/1000], Loss: 3967.2456\n",
            "Epoch [420/1000], Loss: 2997.4849\n",
            "Epoch [420/1000], Loss: 2641.8838\n",
            "Epoch [420/1000], Loss: 3229.1042\n",
            "Epoch [420/1000], Loss: 2997.4719\n",
            "Epoch [420/1000], Loss: 3460.6755\n",
            "Epoch [420/1000], Loss: 4346.022\n",
            "Epoch [420/1000], Loss: 5139.7915\n",
            "Epoch [430/1000], Loss: 3414.0984\n",
            "Epoch [430/1000], Loss: 3173.6499\n",
            "Epoch [430/1000], Loss: 3328.2043\n",
            "Epoch [430/1000], Loss: 3838.5312\n",
            "Epoch [430/1000], Loss: 3168.6758\n",
            "Epoch [430/1000], Loss: 2924.8542\n",
            "Epoch [430/1000], Loss: 3738.4575\n",
            "Epoch [430/1000], Loss: 3996.6277\n",
            "Epoch [440/1000], Loss: 3847.9363\n",
            "Epoch [440/1000], Loss: 2915.54\n",
            "Epoch [440/1000], Loss: 3045.4497\n",
            "Epoch [440/1000], Loss: 2691.248\n",
            "Epoch [440/1000], Loss: 3626.7324\n",
            "Epoch [440/1000], Loss: 4196.8257\n",
            "Epoch [440/1000], Loss: 3038.79\n",
            "Epoch [440/1000], Loss: 6364.3984\n",
            "Epoch [450/1000], Loss: 3247.8391\n",
            "Epoch [450/1000], Loss: 4480.8379\n",
            "Epoch [450/1000], Loss: 2938.6006\n",
            "Epoch [450/1000], Loss: 2935.0159\n",
            "Epoch [450/1000], Loss: 3373.7705\n",
            "Epoch [450/1000], Loss: 2994.2219\n",
            "Epoch [450/1000], Loss: 3527.2075\n",
            "Epoch [450/1000], Loss: 2020.7518\n",
            "Epoch [460/1000], Loss: 2801.2893\n",
            "Epoch [460/1000], Loss: 4470.9014\n",
            "Epoch [460/1000], Loss: 2992.7681\n",
            "Epoch [460/1000], Loss: 2696.6208\n",
            "Epoch [460/1000], Loss: 3555.4805\n",
            "Epoch [460/1000], Loss: 3531.4937\n",
            "Epoch [460/1000], Loss: 3374.574\n",
            "Epoch [460/1000], Loss: 1542.3314\n",
            "Epoch [470/1000], Loss: 3965.3796\n",
            "Epoch [470/1000], Loss: 3358.7244\n",
            "Epoch [470/1000], Loss: 2502.0261\n",
            "Epoch [470/1000], Loss: 3247.9749\n",
            "Epoch [470/1000], Loss: 3116.0737\n",
            "Epoch [470/1000], Loss: 3570.8059\n",
            "Epoch [470/1000], Loss: 3629.573\n",
            "Epoch [470/1000], Loss: 473.3839\n",
            "Epoch [480/1000], Loss: 3850.4929\n",
            "Epoch [480/1000], Loss: 3878.7712\n",
            "Epoch [480/1000], Loss: 2911.6836\n",
            "Epoch [480/1000], Loss: 3439.5171\n",
            "Epoch [480/1000], Loss: 2866.4158\n",
            "Epoch [480/1000], Loss: 2919.9163\n",
            "Epoch [480/1000], Loss: 3306.2131\n",
            "Epoch [480/1000], Loss: 2475.8237\n",
            "Epoch [490/1000], Loss: 2722.3838\n",
            "Epoch [490/1000], Loss: 2462.7473\n",
            "Epoch [490/1000], Loss: 3261.9055\n",
            "Epoch [490/1000], Loss: 3146.8855\n",
            "Epoch [490/1000], Loss: 3446.2917\n",
            "Epoch [490/1000], Loss: 4303.4131\n",
            "Epoch [490/1000], Loss: 3703.9277\n",
            "Epoch [490/1000], Loss: 2892.5728\n",
            "Epoch [500/1000], Loss: 2511.8064\n",
            "Epoch [500/1000], Loss: 3768.3181\n",
            "Epoch [500/1000], Loss: 3919.7395\n",
            "Epoch [500/1000], Loss: 3049.9817\n",
            "Epoch [500/1000], Loss: 2853.0251\n",
            "Epoch [500/1000], Loss: 3008.9624\n",
            "Epoch [500/1000], Loss: 3580.7969\n",
            "Epoch [500/1000], Loss: 7240.2983\n",
            "Epoch [510/1000], Loss: 2878.22\n",
            "Epoch [510/1000], Loss: 3708.1343\n",
            "Epoch [510/1000], Loss: 2845.7549\n",
            "Epoch [510/1000], Loss: 2901.9436\n",
            "Epoch [510/1000], Loss: 3014.9348\n",
            "Epoch [510/1000], Loss: 4718.1348\n",
            "Epoch [510/1000], Loss: 2862.8276\n",
            "Epoch [510/1000], Loss: 1768.3892\n",
            "Epoch [520/1000], Loss: 3774.7593\n",
            "Epoch [520/1000], Loss: 3543.5977\n",
            "Epoch [520/1000], Loss: 3534.6624\n",
            "Epoch [520/1000], Loss: 2806.9653\n",
            "Epoch [520/1000], Loss: 2734.4434\n",
            "Epoch [520/1000], Loss: 2720.7705\n",
            "Epoch [520/1000], Loss: 3531.5718\n",
            "Epoch [520/1000], Loss: 5129.6694\n",
            "Epoch [530/1000], Loss: 2541.9392\n",
            "Epoch [530/1000], Loss: 3392.8169\n",
            "Epoch [530/1000], Loss: 4184.2075\n",
            "Epoch [530/1000], Loss: 3332.6155\n",
            "Epoch [530/1000], Loss: 2790.043\n",
            "Epoch [530/1000], Loss: 2945.7783\n",
            "Epoch [530/1000], Loss: 3417.718\n",
            "Epoch [530/1000], Loss: 4328.959\n",
            "Epoch [540/1000], Loss: 4658.2378\n",
            "Epoch [540/1000], Loss: 2405.5261\n",
            "Epoch [540/1000], Loss: 2964.4724\n",
            "Epoch [540/1000], Loss: 3909.4556\n",
            "Epoch [540/1000], Loss: 2788.9436\n",
            "Epoch [540/1000], Loss: 3880.1836\n",
            "Epoch [540/1000], Loss: 2016.1853\n",
            "Epoch [540/1000], Loss: 2468.3115\n",
            "Epoch [550/1000], Loss: 3771.5393\n",
            "Epoch [550/1000], Loss: 3580.1375\n",
            "Epoch [550/1000], Loss: 3350.3875\n",
            "Epoch [550/1000], Loss: 3212.0173\n",
            "Epoch [550/1000], Loss: 2799.7913\n",
            "Epoch [550/1000], Loss: 3365.0017\n",
            "Epoch [550/1000], Loss: 2542.7935\n",
            "Epoch [550/1000], Loss: 1252.4216\n",
            "Epoch [560/1000], Loss: 4296.0347\n",
            "Epoch [560/1000], Loss: 3342.5469\n",
            "Epoch [560/1000], Loss: 2656.3896\n",
            "Epoch [560/1000], Loss: 3505.0442\n",
            "Epoch [560/1000], Loss: 2924.5686\n",
            "Epoch [560/1000], Loss: 3285.8838\n",
            "Epoch [560/1000], Loss: 2469.8096\n",
            "Epoch [560/1000], Loss: 2193.188\n",
            "Epoch [570/1000], Loss: 3214.2644\n",
            "Epoch [570/1000], Loss: 3831.7617\n",
            "Epoch [570/1000], Loss: 3559.9468\n",
            "Epoch [570/1000], Loss: 1658.6256\n",
            "Epoch [570/1000], Loss: 3223.8772\n",
            "Epoch [570/1000], Loss: 3732.3191\n",
            "Epoch [570/1000], Loss: 3161.4246\n",
            "Epoch [570/1000], Loss: 2606.3091\n",
            "Epoch [580/1000], Loss: 3357.8711\n",
            "Epoch [580/1000], Loss: 2981.5461\n",
            "Epoch [580/1000], Loss: 3136.2786\n",
            "Epoch [580/1000], Loss: 3172.998\n",
            "Epoch [580/1000], Loss: 2663.1875\n",
            "Epoch [580/1000], Loss: 3208.6572\n",
            "Epoch [580/1000], Loss: 3822.9448\n",
            "Epoch [580/1000], Loss: 2065.2524\n",
            "Epoch [590/1000], Loss: 3286.1208\n",
            "Epoch [590/1000], Loss: 2811.8582\n",
            "Epoch [590/1000], Loss: 3166.7905\n",
            "Epoch [590/1000], Loss: 2922.9109\n",
            "Epoch [590/1000], Loss: 3838.345\n",
            "Epoch [590/1000], Loss: 3235.0005\n",
            "Epoch [590/1000], Loss: 3054.103\n",
            "Epoch [590/1000], Loss: 1473.91\n",
            "Epoch [600/1000], Loss: 3604.1519\n",
            "Epoch [600/1000], Loss: 3138.635\n",
            "Epoch [600/1000], Loss: 2434.9143\n",
            "Epoch [600/1000], Loss: 3395.4734\n",
            "Epoch [600/1000], Loss: 2361.3198\n",
            "Epoch [600/1000], Loss: 4088.8816\n",
            "Epoch [600/1000], Loss: 3077.5081\n",
            "Epoch [600/1000], Loss: 3759.1284\n",
            "Epoch [610/1000], Loss: 2816.1311\n",
            "Epoch [610/1000], Loss: 3313.6062\n",
            "Epoch [610/1000], Loss: 3911.6965\n",
            "Epoch [610/1000], Loss: 3194.1453\n",
            "Epoch [610/1000], Loss: 2737.8159\n",
            "Epoch [610/1000], Loss: 3189.1196\n",
            "Epoch [610/1000], Loss: 3027.7126\n",
            "Epoch [610/1000], Loss: 1324.1025\n",
            "Epoch [620/1000], Loss: 3299.6609\n",
            "Epoch [620/1000], Loss: 2485.8765\n",
            "Epoch [620/1000], Loss: 3006.3469\n",
            "Epoch [620/1000], Loss: 2998.0137\n",
            "Epoch [620/1000], Loss: 3540.52\n",
            "Epoch [620/1000], Loss: 3185.7222\n",
            "Epoch [620/1000], Loss: 3573.6018\n",
            "Epoch [620/1000], Loss: 1956.6353\n",
            "Epoch [630/1000], Loss: 3297.4443\n",
            "Epoch [630/1000], Loss: 2739.6506\n",
            "Epoch [630/1000], Loss: 2539.7878\n",
            "Epoch [630/1000], Loss: 3118.0818\n",
            "Epoch [630/1000], Loss: 2770.9243\n",
            "Epoch [630/1000], Loss: 3960.1218\n",
            "Epoch [630/1000], Loss: 3194.0256\n",
            "Epoch [630/1000], Loss: 8523.1592\n",
            "Epoch [640/1000], Loss: 2626.7856\n",
            "Epoch [640/1000], Loss: 3030.6399\n",
            "Epoch [640/1000], Loss: 2858.605\n",
            "Epoch [640/1000], Loss: 3971.8037\n",
            "Epoch [640/1000], Loss: 3082.0073\n",
            "Epoch [640/1000], Loss: 3382.4436\n",
            "Epoch [640/1000], Loss: 3079.6731\n",
            "Epoch [640/1000], Loss: 647.8451\n",
            "Epoch [650/1000], Loss: 3043.74\n",
            "Epoch [650/1000], Loss: 3005.7065\n",
            "Epoch [650/1000], Loss: 2390.4983\n",
            "Epoch [650/1000], Loss: 2570.9402\n",
            "Epoch [650/1000], Loss: 4015.5864\n",
            "Epoch [650/1000], Loss: 3336.1663\n",
            "Epoch [650/1000], Loss: 3450.2261\n",
            "Epoch [650/1000], Loss: 3369.8574\n",
            "Epoch [660/1000], Loss: 2323.1082\n",
            "Epoch [660/1000], Loss: 2581.4502\n",
            "Epoch [660/1000], Loss: 2699.5024\n",
            "Epoch [660/1000], Loss: 3392.7856\n",
            "Epoch [660/1000], Loss: 3032.219\n",
            "Epoch [660/1000], Loss: 4264.2686\n",
            "Epoch [660/1000], Loss: 3385.6218\n",
            "Epoch [660/1000], Loss: 4850.2373\n",
            "Epoch [670/1000], Loss: 3483.3972\n",
            "Epoch [670/1000], Loss: 2508.4709\n",
            "Epoch [670/1000], Loss: 3070.675\n",
            "Epoch [670/1000], Loss: 3022.0017\n",
            "Epoch [670/1000], Loss: 2502.2378\n",
            "Epoch [670/1000], Loss: 3539.9817\n",
            "Epoch [670/1000], Loss: 3523.4111\n",
            "Epoch [670/1000], Loss: 4201.4785\n",
            "Epoch [680/1000], Loss: 2779.0564\n",
            "Epoch [680/1000], Loss: 3937.7278\n",
            "Epoch [680/1000], Loss: 2896.5439\n",
            "Epoch [680/1000], Loss: 3415.5386\n",
            "Epoch [680/1000], Loss: 2295.5071\n",
            "Epoch [680/1000], Loss: 3199.3115\n",
            "Epoch [680/1000], Loss: 3086.1753\n",
            "Epoch [680/1000], Loss: 3938.1909\n",
            "Epoch [690/1000], Loss: 2754.0698\n",
            "Epoch [690/1000], Loss: 3290.6948\n",
            "Epoch [690/1000], Loss: 2442.8643\n",
            "Epoch [690/1000], Loss: 3601.6836\n",
            "Epoch [690/1000], Loss: 3497.1287\n",
            "Epoch [690/1000], Loss: 3163.4622\n",
            "Epoch [690/1000], Loss: 2843.9519\n",
            "Epoch [690/1000], Loss: 3555.8379\n",
            "Epoch [700/1000], Loss: 3300.2312\n",
            "Epoch [700/1000], Loss: 2859.437\n",
            "Epoch [700/1000], Loss: 3490.1155\n",
            "Epoch [700/1000], Loss: 2799.2991\n",
            "Epoch [700/1000], Loss: 2454.343\n",
            "Epoch [700/1000], Loss: 2662.053\n",
            "Epoch [700/1000], Loss: 3895.7783\n",
            "Epoch [700/1000], Loss: 4953.439\n",
            "Epoch [710/1000], Loss: 3128.7314\n",
            "Epoch [710/1000], Loss: 2926.6848\n",
            "Epoch [710/1000], Loss: 3603.0798\n",
            "Epoch [710/1000], Loss: 3353.1775\n",
            "Epoch [710/1000], Loss: 3493.1187\n",
            "Epoch [710/1000], Loss: 2200.801\n",
            "Epoch [710/1000], Loss: 2928.9661\n",
            "Epoch [710/1000], Loss: 995.5648\n",
            "Epoch [720/1000], Loss: 3621.2061\n",
            "Epoch [720/1000], Loss: 3194.3694\n",
            "Epoch [720/1000], Loss: 2884.4775\n",
            "Epoch [720/1000], Loss: 3074.3098\n",
            "Epoch [720/1000], Loss: 3330.1162\n",
            "Epoch [720/1000], Loss: 2641.8293\n",
            "Epoch [720/1000], Loss: 2872.1931\n",
            "Epoch [720/1000], Loss: 729.7314\n",
            "Epoch [730/1000], Loss: 3089.7104\n",
            "Epoch [730/1000], Loss: 2657.8174\n",
            "Epoch [730/1000], Loss: 2768.1267\n",
            "Epoch [730/1000], Loss: 2236.8796\n",
            "Epoch [730/1000], Loss: 3227.0837\n",
            "Epoch [730/1000], Loss: 3966.1038\n",
            "Epoch [730/1000], Loss: 3564.0972\n",
            "Epoch [730/1000], Loss: 1607.3962\n",
            "Epoch [740/1000], Loss: 4292.2642\n",
            "Epoch [740/1000], Loss: 2324.4429\n",
            "Epoch [740/1000], Loss: 3733.7473\n",
            "Epoch [740/1000], Loss: 3210.2903\n",
            "Epoch [740/1000], Loss: 2644.0906\n",
            "Epoch [740/1000], Loss: 2723.3186\n",
            "Epoch [740/1000], Loss: 2505.3467\n",
            "Epoch [740/1000], Loss: 2384.5889\n",
            "Epoch [750/1000], Loss: 2368.271\n",
            "Epoch [750/1000], Loss: 3458.0547\n",
            "Epoch [750/1000], Loss: 3948.9773\n",
            "Epoch [750/1000], Loss: 3365.0693\n",
            "Epoch [750/1000], Loss: 2604.96\n",
            "Epoch [750/1000], Loss: 3156.9353\n",
            "Epoch [750/1000], Loss: 2532.0559\n",
            "Epoch [750/1000], Loss: 1636.9795\n",
            "Epoch [760/1000], Loss: 2738.8755\n",
            "Epoch [760/1000], Loss: 3626.8865\n",
            "Epoch [760/1000], Loss: 3102.1724\n",
            "Epoch [760/1000], Loss: 2527.0808\n",
            "Epoch [760/1000], Loss: 2910.0518\n",
            "Epoch [760/1000], Loss: 2509.793\n",
            "Epoch [760/1000], Loss: 3964.6255\n",
            "Epoch [760/1000], Loss: 1703.0463\n",
            "Epoch [770/1000], Loss: 2805.115\n",
            "Epoch [770/1000], Loss: 2682.8699\n",
            "Epoch [770/1000], Loss: 3650.2671\n",
            "Epoch [770/1000], Loss: 4036.2356\n",
            "Epoch [770/1000], Loss: 2705.96\n",
            "Epoch [770/1000], Loss: 3312.7878\n",
            "Epoch [770/1000], Loss: 2082.271\n",
            "Epoch [770/1000], Loss: 2734.8787\n",
            "Epoch [780/1000], Loss: 2568.3237\n",
            "Epoch [780/1000], Loss: 2844.1274\n",
            "Epoch [780/1000], Loss: 3075.2505\n",
            "Epoch [780/1000], Loss: 2885.9167\n",
            "Epoch [780/1000], Loss: 3115.4255\n",
            "Epoch [780/1000], Loss: 2999.7322\n",
            "Epoch [780/1000], Loss: 3746.095\n",
            "Epoch [780/1000], Loss: 2946.394\n",
            "Epoch [790/1000], Loss: 2551.8413\n",
            "Epoch [790/1000], Loss: 2774.1624\n",
            "Epoch [790/1000], Loss: 4143.5996\n",
            "Epoch [790/1000], Loss: 2652.2449\n",
            "Epoch [790/1000], Loss: 2759.1624\n",
            "Epoch [790/1000], Loss: 3566.9492\n",
            "Epoch [790/1000], Loss: 2792.3\n",
            "Epoch [790/1000], Loss: 2253.5132\n",
            "Epoch [800/1000], Loss: 2637.7983\n",
            "Epoch [800/1000], Loss: 2986.1311\n",
            "Epoch [800/1000], Loss: 3493.4512\n",
            "Epoch [800/1000], Loss: 2786.3008\n",
            "Epoch [800/1000], Loss: 2614.4109\n",
            "Epoch [800/1000], Loss: 2890.8354\n",
            "Epoch [800/1000], Loss: 3771.4006\n",
            "Epoch [800/1000], Loss: 2559.104\n",
            "Epoch [810/1000], Loss: 2761.2856\n",
            "Epoch [810/1000], Loss: 2913.218\n",
            "Epoch [810/1000], Loss: 3386.1875\n",
            "Epoch [810/1000], Loss: 2903.0552\n",
            "Epoch [810/1000], Loss: 3623.2266\n",
            "Epoch [810/1000], Loss: 3021.7202\n",
            "Epoch [810/1000], Loss: 2640.908\n",
            "Epoch [810/1000], Loss: 991.1481\n",
            "Epoch [820/1000], Loss: 3295.8987\n",
            "Epoch [820/1000], Loss: 2585.9714\n",
            "Epoch [820/1000], Loss: 3231.9912\n",
            "Epoch [820/1000], Loss: 2458.5823\n",
            "Epoch [820/1000], Loss: 3847.3096\n",
            "Epoch [820/1000], Loss: 2892.8313\n",
            "Epoch [820/1000], Loss: 2918.2986\n",
            "Epoch [820/1000], Loss: 647.5786\n",
            "Epoch [830/1000], Loss: 2545.2744\n",
            "Epoch [830/1000], Loss: 2809.0769\n",
            "Epoch [830/1000], Loss: 2324.8643\n",
            "Epoch [830/1000], Loss: 3758.7087\n",
            "Epoch [830/1000], Loss: 3206.6238\n",
            "Epoch [830/1000], Loss: 3443.0173\n",
            "Epoch [830/1000], Loss: 3031.3794\n",
            "Epoch [830/1000], Loss: 1953.0804\n",
            "Epoch [840/1000], Loss: 2945.6309\n",
            "Epoch [840/1000], Loss: 2725.8164\n",
            "Epoch [840/1000], Loss: 2717.1606\n",
            "Epoch [840/1000], Loss: 2918.093\n",
            "Epoch [840/1000], Loss: 3310.7869\n",
            "Epoch [840/1000], Loss: 4053.2825\n",
            "Epoch [840/1000], Loss: 2067.1714\n",
            "Epoch [840/1000], Loss: 7848.7183\n",
            "Epoch [850/1000], Loss: 3514.5256\n",
            "Epoch [850/1000], Loss: 3276.2083\n",
            "Epoch [850/1000], Loss: 2684.3003\n",
            "Epoch [850/1000], Loss: 2434.0417\n",
            "Epoch [850/1000], Loss: 2264.0881\n",
            "Epoch [850/1000], Loss: 3565.9348\n",
            "Epoch [850/1000], Loss: 3338.0107\n",
            "Epoch [850/1000], Loss: 1637.6805\n",
            "Epoch [860/1000], Loss: 3397.8486\n",
            "Epoch [860/1000], Loss: 3660.7104\n",
            "Epoch [860/1000], Loss: 3083.9355\n",
            "Epoch [860/1000], Loss: 3716.8923\n",
            "Epoch [860/1000], Loss: 3072.8682\n",
            "Epoch [860/1000], Loss: 1892.8895\n",
            "Epoch [860/1000], Loss: 2204.7656\n",
            "Epoch [860/1000], Loss: 2015.8267\n",
            "Epoch [870/1000], Loss: 3660.728\n",
            "Epoch [870/1000], Loss: 2327.1897\n",
            "Epoch [870/1000], Loss: 3010.9783\n",
            "Epoch [870/1000], Loss: 2588.4465\n",
            "Epoch [870/1000], Loss: 3178.7705\n",
            "Epoch [870/1000], Loss: 3772.3545\n",
            "Epoch [870/1000], Loss: 2234.8137\n",
            "Epoch [870/1000], Loss: 5827.7188\n",
            "Epoch [880/1000], Loss: 3164.2161\n",
            "Epoch [880/1000], Loss: 3521.1958\n",
            "Epoch [880/1000], Loss: 2327.4727\n",
            "Epoch [880/1000], Loss: 2903.4077\n",
            "Epoch [880/1000], Loss: 3345.3911\n",
            "Epoch [880/1000], Loss: 2752.4697\n",
            "Epoch [880/1000], Loss: 2780.3843\n",
            "Epoch [880/1000], Loss: 4964.4521\n",
            "Epoch [890/1000], Loss: 3249.0474\n",
            "Epoch [890/1000], Loss: 2963.207\n",
            "Epoch [890/1000], Loss: 3081.9705\n",
            "Epoch [890/1000], Loss: 2055.4587\n",
            "Epoch [890/1000], Loss: 3438.5967\n",
            "Epoch [890/1000], Loss: 3547.2192\n",
            "Epoch [890/1000], Loss: 2628.7815\n",
            "Epoch [890/1000], Loss: 1727.3816\n",
            "Epoch [900/1000], Loss: 2173.2068\n",
            "Epoch [900/1000], Loss: 3401.687\n",
            "Epoch [900/1000], Loss: 3117.6536\n",
            "Epoch [900/1000], Loss: 2570.1484\n",
            "Epoch [900/1000], Loss: 4000.0381\n",
            "Epoch [900/1000], Loss: 2634.9062\n",
            "Epoch [900/1000], Loss: 3124.7937\n",
            "Epoch [900/1000], Loss: 681.4539\n",
            "Epoch [910/1000], Loss: 2659.3462\n",
            "Epoch [910/1000], Loss: 2596.4275\n",
            "Epoch [910/1000], Loss: 3321.3665\n",
            "Epoch [910/1000], Loss: 2521.0684\n",
            "Epoch [910/1000], Loss: 3779.8455\n",
            "Epoch [910/1000], Loss: 3045.6033\n",
            "Epoch [910/1000], Loss: 2716.4812\n",
            "Epoch [910/1000], Loss: 6681.8877\n",
            "Epoch [920/1000], Loss: 2696.2324\n",
            "Epoch [920/1000], Loss: 3128.7134\n",
            "Epoch [920/1000], Loss: 3268.6211\n",
            "Epoch [920/1000], Loss: 2195.238\n",
            "Epoch [920/1000], Loss: 2504.2983\n",
            "Epoch [920/1000], Loss: 3777.7183\n",
            "Epoch [920/1000], Loss: 2779.3894\n",
            "Epoch [920/1000], Loss: 10769.7471\n",
            "Epoch [930/1000], Loss: 4203.9736\n",
            "Epoch [930/1000], Loss: 2832.5601\n",
            "Epoch [930/1000], Loss: 2138.5967\n",
            "Epoch [930/1000], Loss: 3329.3276\n",
            "Epoch [930/1000], Loss: 2211.8428\n",
            "Epoch [930/1000], Loss: 2760.4902\n",
            "Epoch [930/1000], Loss: 3488.6953\n",
            "Epoch [930/1000], Loss: 138.0418\n",
            "Epoch [940/1000], Loss: 1921.1353\n",
            "Epoch [940/1000], Loss: 3490.1814\n",
            "Epoch [940/1000], Loss: 3139.7856\n",
            "Epoch [940/1000], Loss: 2761.269\n",
            "Epoch [940/1000], Loss: 3405.6458\n",
            "Epoch [940/1000], Loss: 3164.3447\n",
            "Epoch [940/1000], Loss: 2881.6467\n",
            "Epoch [940/1000], Loss: 3151.2988\n",
            "Epoch [950/1000], Loss: 2891.8433\n",
            "Epoch [950/1000], Loss: 2636.6406\n",
            "Epoch [950/1000], Loss: 3738.2153\n",
            "Epoch [950/1000], Loss: 3646.468\n",
            "Epoch [950/1000], Loss: 2207.5378\n",
            "Epoch [950/1000], Loss: 2233.2324\n",
            "Epoch [950/1000], Loss: 3576.2012\n",
            "Epoch [950/1000], Loss: 864.5177\n",
            "Epoch [960/1000], Loss: 3476.9009\n",
            "Epoch [960/1000], Loss: 3428.7517\n",
            "Epoch [960/1000], Loss: 2448.5657\n",
            "Epoch [960/1000], Loss: 4038.949\n",
            "Epoch [960/1000], Loss: 2243.6233\n",
            "Epoch [960/1000], Loss: 2339.8142\n",
            "Epoch [960/1000], Loss: 2939.1082\n",
            "Epoch [960/1000], Loss: 814.0674\n",
            "Epoch [970/1000], Loss: 2326.2732\n",
            "Epoch [970/1000], Loss: 3551.823\n",
            "Epoch [970/1000], Loss: 2781.8901\n",
            "Epoch [970/1000], Loss: 2657.2156\n",
            "Epoch [970/1000], Loss: 3038.0125\n",
            "Epoch [970/1000], Loss: 3134.1609\n",
            "Epoch [970/1000], Loss: 3366.8799\n",
            "Epoch [970/1000], Loss: 1257.9069\n",
            "Epoch [980/1000], Loss: 2770.6675\n",
            "Epoch [980/1000], Loss: 3179.9177\n",
            "Epoch [980/1000], Loss: 2317.9246\n",
            "Epoch [980/1000], Loss: 2332.2122\n",
            "Epoch [980/1000], Loss: 3734.9521\n",
            "Epoch [980/1000], Loss: 2430.8018\n",
            "Epoch [980/1000], Loss: 3398.072\n",
            "Epoch [980/1000], Loss: 12033.1836\n",
            "Epoch [990/1000], Loss: 2579.0679\n",
            "Epoch [990/1000], Loss: 2866.8794\n",
            "Epoch [990/1000], Loss: 2544.2195\n",
            "Epoch [990/1000], Loss: 3503.8037\n",
            "Epoch [990/1000], Loss: 2947.8853\n",
            "Epoch [990/1000], Loss: 3292.7166\n",
            "Epoch [990/1000], Loss: 2965.0256\n",
            "Epoch [990/1000], Loss: 2968.5073\n",
            "Epoch [1000/1000], Loss: 2725.0847\n",
            "Epoch [1000/1000], Loss: 3559.4866\n",
            "Epoch [1000/1000], Loss: 2368.2463\n",
            "Epoch [1000/1000], Loss: 3818.1824\n",
            "Epoch [1000/1000], Loss: 2898.9443\n",
            "Epoch [1000/1000], Loss: 3415.6067\n",
            "Epoch [1000/1000], Loss: 1935.422\n",
            "Epoch [1000/1000], Loss: 2119.4316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() # testing mode\n",
        "mse_values = [] # collect the MSE scores\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs) # predict the test data\n",
        "\n",
        "        # Calculate Mean Squared Error\n",
        "        mse = criterion(outputs, targets) # calcualte mse for the batch\n",
        "        mse_values.append(mse.item()) # add to the list of MSE values\n",
        "\n",
        "# Calculate and print the average MSE\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")\n",
        "\n",
        "# Evaluation for predictions and actuals\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({'Predicted': np.array(predictions).flatten(), 'Actual': np.array(actuals).flatten()})\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBTxMHdYP0Li",
        "outputId": "24f26645-e955-4f9b-f2aa-6f0e45447101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 2848.207275390625\n",
            "     Predicted  Actual\n",
            "0   146.816254   219.0\n",
            "1   174.319839    70.0\n",
            "2   143.888123   202.0\n",
            "3   292.759125   230.0\n",
            "4   128.393600   111.0\n",
            "..         ...     ...\n",
            "84  116.534256   153.0\n",
            "85   88.764580    98.0\n",
            "86   76.190804    37.0\n",
            "87   65.578255    63.0\n",
            "88  151.681793   184.0\n",
            "\n",
            "[89 rows x 2 columns]\n"
          ]
        }
      ]
    }
  ]
}